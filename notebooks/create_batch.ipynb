{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import Workspace\r\n",
    "ws = Workspace.from_config()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Provision compute context for the pipeline\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.core.compute_target import ComputeTargetException\r\n",
    "\r\n",
    "cluster_name = \"mlopsbootcamp\"\r\n",
    "\r\n",
    "try:\r\n",
    "    # Check for existing compute target\r\n",
    "    inference_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
    "    print('Found existing cluster, use it.')\r\n",
    "except ComputeTargetException:\r\n",
    "    # If it doesn't already exist, create it\r\n",
    "    try:\r\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS2_v2', max_nodes=2)\r\n",
    "        inference_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
    "        inference_cluster.wait_for_completion(show_output=True)\r\n",
    "    except Exception as ex:\r\n",
    "        print(ex)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "\r\n",
    "# Create a folder for the files\r\n",
    "capstone_folder = os.path.join(os.path.dirname(os.getcwd()),'pytown_energymonitor')\r\n",
    "print(capstone_folder)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "%%writefile $capstone_folder\\score.py\r\n",
    "\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "def init():\r\n",
    "    # Runs when the pipeline step is initialized\r\n",
    "    global model\r\n",
    "\r\n",
    "\r\n",
    "def get_classification(df_pred, col_energy, col_prediction):\r\n",
    "    df_pred['difference'] = df_pred[col_energy] - df_pred[col_prediction]\r\n",
    "    df_pred['class'] = 0 # add a class column with 0 as low charge (1 is middle charge  and 2 is normal charge)\r\n",
    "    df_pred.loc[df_pred['difference'] > 0, 'class'] = 1 \r\n",
    "    df_pred.loc[df_pred['difference'] > 5000, 'class'] = 2 \r\n",
    "    return df_pred\r\n",
    "\r\n",
    "def run(mini_batch):\r\n",
    "    # mini_batch is the pandas dataframe    \r\n",
    "  \r\n",
    "    #calculate naive forecast\r\n",
    "    prediction_df = (\r\n",
    "        mini_batch\r\n",
    "        .set_index('data_index_')\r\n",
    "        .groupby('dayofweek')\r\n",
    "        .load_actuals_mw\r\n",
    "        .rolling(3, closed = 'left')\r\n",
    "        .mean()\r\n",
    "        .reset_index()\r\n",
    "    ).rename(columns ={'load_actuals_mw' : 'load_pred_mw'})\r\n",
    "\r\n",
    "    df_complete = (\r\n",
    "        pd.merge(prediction_df, mini_batch, on='data_index_')\r\n",
    "        .drop('dayofweek_y', axis=1)\r\n",
    "        .sort_values(by='data_index_')\r\n",
    "        .set_index('data_index_')\r\n",
    "    )\r\n",
    "\r\n",
    "    return get_classification(df_complete, 'total_gen', 'load_pred_mw')\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting c:\\Users\\meira\\Projects\\PyLadiesMLOpsCapstone\\pytown_energymonitor\\score.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "%%writefile $capstone_folder\\capstone_environment.yml\r\n",
    "name: capstone_environment\r\n",
    "dependencies:\r\n",
    "- python=3.8\r\n",
    "- numpy\r\n",
    "- pandas\r\n",
    "- scikit-learn\r\n",
    "- pip:\r\n",
    "  - azureml-core\r\n",
    "  - azureml-dataset-runtime[fuse]\r\n",
    "  - azureml-pipeline-core\r\n",
    "  - azureml-pipeline-steps\r\n",
    "  - azureml-dataprep"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting c:\\Users\\meira\\Projects\\PyLadiesMLOpsCapstone\\pytown_energymonitor\\capstone_environment.yml\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from azureml.core import Environment\r\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\r\n",
    "\r\n",
    "# Create an Environment for the pipeline to run\r\n",
    "capstone_env = Environment.from_conda_specification(\"capstone_env\", capstone_folder + \"/capstone_environment.yml\")\r\n",
    "capstone_env.docker.base_image = DEFAULT_CPU_IMAGE\r\n",
    "print('Configuration ready.')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Configuration ready.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# run the batch pipeline: execute py script and save results to txt in the output folder\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\r\n",
    "from azureml.data import OutputFileDatasetConfig\r\n",
    "from azureml.core.runconfig import DockerConfiguration\r\n",
    "\r\n",
    "# Get the batch dataset for input\r\n",
    "input_data_set = ws.datasets['daily_load_and_wind'] # Tabular dataset -> this means the mini-batch is a pd.DataFrame\r\n",
    "\r\n",
    "# Set the output location\r\n",
    "default_ds = ws.get_default_datastore()\r\n",
    "output_dir = OutputFileDatasetConfig(name='capstone_inferences')\r\n",
    "\r\n",
    "# Define the parallel run step step configuration\r\n",
    "parallel_run_config = ParallelRunConfig(\r\n",
    "    source_directory=capstone_folder,\r\n",
    "    entry_script=\"score.py\",\r\n",
    "    mini_batch_size=\"10MB\",\r\n",
    "    error_threshold=10,\r\n",
    "    output_action=\"append_row\",\r\n",
    "    environment=capstone_env,\r\n",
    "    compute_target=inference_cluster,\r\n",
    "    node_count=2)\r\n",
    "\r\n",
    "parallel_step_name = \"batchscoring-\" + datetime.now().strftime(\"%Y%m%d%H%M\")\r\n",
    "\r\n",
    "# Create the parallel run step\r\n",
    "parallelrun_step = ParallelRunStep(\r\n",
    "    name=parallel_step_name,\r\n",
    "    parallel_run_config=parallel_run_config,\r\n",
    "    inputs=[input_data_set.as_named_input('daily_load_and_wind')],\r\n",
    "    output=output_dir,\r\n",
    "    arguments=[],\r\n",
    "    allow_reuse=True\r\n",
    ")\r\n",
    "\r\n",
    "print('Steps defined')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Steps defined\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\meira\\anaconda3\\envs\\capstone_mlops\\lib\\site-packages\\azureml\\pipeline\\core\\_parallel_run_step_base.py:580: UserWarning: \n",
      "ParallelRunStep requires azureml-dataset-runtime[fuse,pandas] for tabular dataset.\n",
      "Please add relevant package in CondaDependencies.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "from azureml.core import Experiment\r\n",
    "from azureml.pipeline.core import Pipeline\r\n",
    "\r\n",
    "# Create the pipeline\r\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\r\n",
    "\r\n",
    "# Run the pipeline as an experiment\r\n",
    "pipeline_run = Experiment(ws, 'capstone-naive-forecast-batch').submit(pipeline)\r\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import shutil\r\n",
    "\r\n",
    "# Remove the local results folder if left over from a previous run\r\n",
    "shutil.rmtree('capstone-batch-results', ignore_errors=True)\r\n",
    "\r\n",
    "# Get the run for the first step and download its output\r\n",
    "prediction_run = next(pipeline_run.get_children())\r\n",
    "prediction_output = prediction_run.get_output_data('capstone_inferences')\r\n",
    "prediction_output.download(local_path='capstone-batch-results')\r\n",
    "\r\n",
    "# Traverse the folder hierarchy and find the results file\r\n",
    "for root, dirs, files in os.walk('capstone-batch-results'):\r\n",
    "    for file in files:\r\n",
    "        if file.endswith('parallel_run_step.txt'):\r\n",
    "            result_file = os.path.join(root,file)\r\n",
    "\r\n",
    "# cleanup output format\r\n",
    "df = pd.read_csv(result_file, delimiter=\":\", header=None)\r\n",
    "df.columns = [\"File\", \"Prediction\"]\r\n",
    "\r\n",
    "# Display the first 20 results\r\n",
    "df.head(20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# publish the pipeline\r\n",
    "published_pipeline = pipeline_run.publish_pipeline(name='Linear_regression_batch_prediction_pipeline',\r\n",
    "                                                   description='Batch scoring using linear regression model',\r\n",
    "                                                   version='1.0')\r\n",
    "\r\n",
    "published_pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check all published pipelines\r\n",
    "from azureml.pipeline.core import PublishedPipeline\r\n",
    "\r\n",
    "published_pipelines = PublishedPipeline.list(ws)\r\n",
    "for published_pipeline in  published_pipelines:\r\n",
    "    print(f\"{published_pipeline.name},'{published_pipeline.id}'\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# schedule the pipeline\r\n",
    "from azureml.pipeline.core import ScheduleRecurrence, Schedule\r\n",
    "\r\n",
    "\r\n",
    "recurrence = ScheduleRecurrence(frequency='Daily', interval=1)\r\n",
    "pipeline_schedule = Schedule.create(ws, name='Daily Naive Predictions',\r\n",
    "                                        description='capstone naive forecast batch inferencing',\r\n",
    "                                        pipeline_id=published_pipeline.id,\r\n",
    "                                        experiment_name='Capstone_Batch_Prediction',\r\n",
    "                                        recurrence=recurrence)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('capstone_mlops': conda)"
  },
  "interpreter": {
   "hash": "262afc0293dc3ae75c593851c5492fec85b6d8ae246ad1202c627cb3da9b24dc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}